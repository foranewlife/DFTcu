# CUDA 编程手册系列第五章: 性能指南

本章通过概述 CUDA 编程模型是如何在 C++ 中公开的，来介绍 CUDA 的主要概念。

## 5.1 整体性能优化策略

性能优化围绕四个基本策略：
1. **最大化并行执行**以实现最大利用率；
2. **优化内存使用**，实现最大内存吞吐量；
3. **优化指令使用**，实现最大指令吞吐量；
4. **尽量减少内存抖动**。

哪些策略将为应用程序的特定部分产生最佳性能增益取决于该部分的性能限值。例如，优化主要受内存访问限制的内核的指令使用不会产生任何显着的性能提升。因此，应该通过测量和监控性能限制来不断地指导优化工作，例如使用 CUDA 分析器。

此外，将特定内核的浮点运算吞吐量或内存吞吐量（以更有意义的为准）与设备的相应峰值理论吞吐量进行比较表明内核还有多少改进空间。

---

## 5.2 最大化利用率

为了最大限度地提高利用率，应用程序的结构应该尽可能多地暴露并行性，并有效地将这种并行性映射到系统的各个组件，以使它们大部分时间都处于忙碌状态。

### 5.2.1 应用程序层次
在高层次上，应用程序应该通过使用异步流（Streams）来最大化主机、设备和将主机连接到设备的总线之间的并行执行。它应该为每个处理器分配它最擅长的工作类型：
- **主机**：串行工作负载。
- **设备**：并行工作负载。

对于并行工作负载，在算法中由于某些线程需要同步以相互共享数据而破坏并行性的点，有两种情况：
1. **同块线程**：应使用 `__syncthreads()` 并通过共享内存共享数据。
2. **异块线程**：必须使用两个单独的内核调用通过全局内存共享数据。这不太理想，因为它增加了额外内核调用和全局内存流量的开销。

### 5.2.2 设备层次
在较低级别，应用程序应该最大化设备多处理器（SM）之间的并行执行。多个内核可以在一个设备上并发执行，因此也可以通过使用流来启用足够多的内核来实现最大利用率。

### 5.2.3 多处理器层次
在更低的层次上，应用程序应该最大化多处理器内不同功能单元之间的并行执行。GPU 多处理器主要依靠线程级并行性来最大限度地利用其功能单元。因此，利用率与常驻 warp 的数量直接相关。

隐藏 $L$ 个时钟周期延迟所需指令数：
- **计算能力 5.x, 6.1, 6.2, 7.x, 8.x**：$4L$。
- **计算能力 6.0**：$2L$。
- **计算能力 3.x**：$8L$。

**占用率影响因素：**
- **寄存器使用**：内核使用的寄存器数量会对驻留 warp 的数量产生重大影响。
- **共享内存使用**：一个块所需的共享内存总量等于静态和动态分配之和。

#### 5.2.3.1 占用率计算
可以使用 `cudaOccupancyMaxActiveBlocksPerMultiprocessor` 等 API 来预测占用率。

**代码示例：计算占用率**
```cpp
// Device code
__global__ void MyKernel(int *d, int *a, int *b) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    d[idx] = a[idx] * b[idx];
}

// Host code
int main() {
    int numBlocks;
    int blockSize = 32;
    int device;
    cudaDeviceProp prop;
    cudaGetDevice(&device);
    cudaGetDeviceProperties(&prop, device);

    cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlocks, MyKernel, blockSize, 0);

    int activeWarps = numBlocks * blockSize / prop.warpSize;
    int maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;
    std::cout << "Occupancy: " << (double)activeWarps / maxWarps * 100 << "%" << std::endl;
    return 0;
}
```

---

## 5.3 最大化存储吞吐量

最大化内存吞吐量的第一步是最小化低带宽的数据传输。

### 5.3.1 设备与主机之间的数据传输
- 尽量减少主机和设备之间的数据传输，将更多代码移动到设备上。
- 将许多小传输批处理为单个大传输。
- 使用页锁定主机内存（Pinned Memory）实现更高性能。

### 5.3.2 设备内存访问
- **全局内存（Global Memory）**：驻留在设备内存中，通过 32、64 或 128 字节内存事务访问。必须自然对齐。
- **本地内存（Local Memory）**：发生于自动变量溢出寄存器时。具有与全局内存相同的高延迟和低带宽。
- **共享内存（Shared Memory）**：片上存储，带宽极高，延迟极低。需避免 **Bank 冲突（Bank Conflicts）**。
- **常量内存（Constant Memory）**：驻留在设备内存并缓存在常量缓存中。
- **纹理和表面内存（Texture and Surface Memory）**：针对 2D 空间局部性进行了优化。

---

## 5.4 最大化指令吞吐量

为了最大化指令吞吐量，应用程序应该：
1. 尽量减少使用低吞吐量的算术指令。
2. 最小化由控制流指令引起的发散（Divergent）warp。
3. 减少指令的数量（如优化同步点）。

### 5.4.1 算数指令
- **单精度除法**：`__fdividef(x, y)` 比常规除法运算符更快。
- **平方根倒数**：建议直接调用 `rsqrtf()`。
- **三角函数**：`sinf(x)`、`cosf(x)` 等在参数很大时会有“慢速路径”，吞吐量降低一个数量级。
- **整数算术**：除法和模运算成本很高，若 $n$ 为 2 的幂，建议使用按位运算。
- **半精度运算**：建议将 `half2` 或 `__nv_bfloat162` 用于单指令双操作。

### 5.4.2 控制流指令
任何流控制指令（`if`, `switch`, `for`, `while`）若导致相同 warp 的线程发散（Diverge），会显著影响有效指令吞吐量。应编写控制条件以最小化发散 warp 的数量。

### 5.4.3 同步指令
`__syncthreads()` 的吞吐量因计算能力而异：
- 3.x: 128 次操作/周期。
- 6.0: 32 次操作/周期。
- 7.x/8.x: 16 次操作/周期。

---

## 5.5 最小化内存抖动

频繁地分配和释放内存会导致性能下降。建议：
1. 根据问题调整分配大小。
2. 尝试在应用程序早期分配适当大小的内存，仅在不再使用时释放。
3. 减少 `cudaMalloc + cudaFree` 的调用次数。
4. 考虑使用 `cudaMallocManaged`（统一内存），它支持超额订阅并能减少系统调度压力。
